. [[man_install3_step17]] If the version of ONTAP installed on node3 is the same or later than the version of ONTAP 9 installed on node1, list and reassign disks to the new node3:
+
`boot_ontap`
+
WARNING: If this new node has ever been used in any other cluster or HA pair, you must run `wipeconfig` before proceeding. Failure to do so might result in service outages or data loss. Contact technical support if the replacement controller was previously used, especially if the controllers were running ONTAP running in 7-Mode.

. [[man_install3_step18]]Press CTRL-C to display the boot menu.

. [[man_install3_step19]]Take one of the following actions:
+
[cols="35,65"]
|===
|If the system you are upgrading... |Then...

|Does _not_ have the correct or current ONTAP version on node3
|Go to <<man_install3_step20,Step 20>>.
|Has the correct or current version of ONTAP on node3
|Go to <<man_install3_step25,Step 25>>.
|===

. [[man_install3_step20]]Configure the netboot connection by choosing one of the following actions.
+
NOTE: You must use the management port and IP as the netboot connection. Do not use a data LIF IP or else a data outage might occur while the upgrade is being performed.
+
[cols=2*,options="header",cols="35,65"]
|===
|If Dynamic Host Configuration Protocol (DHCP) is... |Then...
|Running
|Configure the connection automatically by entering the following command at the boot environment prompt:
`ifconfig e0M -auto`
|Not running
a|Manually configure the connection by entering the following command at the boot environment prompt:
`ifconfig e0M -addr=_filer_addr_ -mask=_netmask_ -gw=_gateway_ -dns=_dns_addr_ -domain=_dns_domain_`

`_filer_addr_` is the IP address of the storage system (mandatory). 
`_netmask_` is the network mask of the storage system (mandatory).
`_gateway_` is the gateway for the storage system (mandatory).
`_dns_addr_` is the IP address of a name server on your network (optional).
`_dns_domain_` is the Domain Name Service (DNS) domain name. If you use this optional parameter, you do not need a fully qualified domain name in the netboot server URL; you need only the server's host name.

NOTE: Other parameters might be necessary for your interface. Enter `help ifconfig` at the firmware prompt for details.
|===

. [[man_install3_step21]]Perform netboot on node3:
+
[cols="35,65"]
|===
|For... |Then...


|FAS/AFF8000 series systems
|`netboot \http://<web_server_ip>/<path_to_webaccessible_directory>/netboot/kernel`
|All other systems
|`netboot \http://<web_server_ip>/<path_to_webaccessible_directory>/<ontap_version>_image.tgz`
|===
+
The `<path_to_the_web-accessible_directory>` leads to where you downloaded the `<ontap_version>_image.tgz` in link:prepare_for_netboot.html#man_netboot_Step1[Step 1] in the section _Prepare for netboot_.
+
NOTE: Do not interrupt the boot.

. [[man_install3_step22]]From the boot menu, select option *(7) Install new software* first.
+
This menu option downloads and installs the new ONTAP image to the boot device.
+
Disregard the following message:
+
`This procedure is not supported for Non-Disruptive Upgrade on an HA pair`
+
The note applies to nondisruptive upgrades of ONTAP, and not upgrades of controllers.
+
NOTE: Always use netboot to update the new node to the desired image. If you use another method to install the image on the new controller, the wrong image might install. This issue applies to all releases of ONTAP. The netboot procedure combined with option `(7) Install new software` wipes the boot media and places the same ONTAP version ONTAP on both image partitions.

. [[man_install3_step23]]If you are prompted to continue the procedure, enter `y`, and when prompted for the package, enter the following URL:
+
`\http://<web_server_ip>/<path_to_web-accessible_directory>/<ontap_version_image>.tgz`

. [[man_install3_step24]]Complete the following substeps:
+
.. Enter `n` to skip the backup recovery when you see the following prompt:
+
----
Do you want to restore the backup configuration now? {y|n}
----

.. Reboot by entering `y` when you see the following prompt:
+
----
The node must be rebooted to start using the newly installed software. Do you want to reboot now? {y|n}
----
+
The controller module reboots but stops at the boot menu because the boot device was reformatted and the configuration data needs to be restored.

. [[man_install3_step25]]Select *(5) Maintenance mode boot* by entering `5`, and then enter `y` when prompted to continue with the boot.
. [[man_install3_step26]]Before continuing, go to link:set_fc_uta_uta2_config_node3.html[Set the FC or UTA/UTA2 configuration on node3] to make any necessary changes to the FC or UTA/UTA2 ports on the node.
+
Make the changes recommended in those sections, reboot the node, and go into maintenance mode.

. [[man_install3_step27]]Find the system ID of node3:
+
`disk show -a`
+
The system displays the system ID of the node and information about its disks, as shown in the following example:
+
----
 *> disk show -a
 Local System ID: 536881109
 DISK     OWNER                    POOL  SERIAL   HOME          DR
 HOME                                    NUMBER
 -------- -------------            ----- -------- ------------- -------------
 0b.02.23 nst-fas2520-2(536880939) Pool0 KPG2RK6F nst-fas2520-2(536880939)
 0b.02.13 nst-fas2520-2(536880939) Pool0 KPG3DE4F nst-fas2520-2(536880939)
 0b.01.13 nst-fas2520-2(536880939) Pool0 PPG4KLAA nst-fas2520-2(536880939)
 ......
 0a.00.0               (536881109) Pool0 YFKSX6JG              (536881109)
 ......
----
+
NOTE: You might see the message `disk show: No disks match option -a.` after entering the command. This is not an error message so you can continue with the procedure.

. [[man_install3_step28]]Reassign node1's spares, any disks belonging to the root, and any non-root aggregates that were not relocated to node2 earlier in link:relocate_non_root_aggr_node1_node2.html[Relocate non-root aggregates from node1 to node2].
+
Enter the appropriate form of the `disk reassign` command based on whether your system has shared disks:
+
NOTE: If you have shared disks, hybrid aggregates, or both on your system, you must use the correct `disk reassign` command from the following table.
+
[cols="35,65"]
|===
|If disk type is... |Then run the command...

|With shared disks
|`disk reassign -s _node1_sysid_ -d _node3_sysid_ -p _node2_sysid_`
|Without shared disks
|`disk reassign -s _node1_sysid_ -d _node3_sysid_`
|===
+
For the `_node1_sysid_` value, use the information captured in link:record_node1_information.html[Record node1 information]. To obtain the value for `_node3_sysid_`, use the `sysconfig` command.
+
NOTE: The `-p` option is only required in maintenance mode when shared disks are present.
+
The `disk reassign` command reassigns only those disks for which `_node1_sysid_` is the current owner.
+

The system displays the following message:
+
----
Partner node must not be in Takeover mode during disk reassignment from maintenance mode.
Serious problems could result!!
Do not proceed with reassignment if the partner is in takeover mode. Abort reassignment (y/n)?
----

. [[man_install3_step29]]Enter `n`.
+
The system displays the following message:
+
----
After the node becomes operational, you must perform a takeover and giveback of the HA partner node to ensure disk reassignment is successful.
Do you want to continue (y/n)?
----

. [[man_install3_step30]]Enter `y`
+
The system displays the following message:
+
----
Disk ownership will be updated on all disks previously belonging to Filer with sysid <sysid>.
Do you want to continue (y/n)?
----

. [[man_install3_step31]]Enter `y`.

. [[man_install3_step32]]If you are upgrading from a system with external disks to a system that supports internal and external disks (AFF A800 systems, for example), set the node1 aggregate as root to confirm that node3 boots from the root aggregate of node1.
+
WARNING: *Warning*: You must perform the following substeps in the exact order shown; failure to do so might cause an outage or even data loss.

+
The following procedure sets node3 to boot from the root aggregate of node1:
+
.. Check the RAID, plex, and checksum information for the node1 aggregate:
+
`aggr status -r`

.. Check the status of the node1 aggregate:
+
`aggr status`

.. Bring the node1 aggregate online, if necessary:
+
`aggr_online _root_aggr_from_node1_`

.. Prevent the node3 from booting from its original root aggregate:
`aggr offline _root_aggr_on_node3_`

.. Set the node1 root aggregate as the new root aggregate for node3:
+
`aggr options _aggr_from_node1_ root`

.. Verify that the root aggregate of node3 is offline and the root aggregate for the disks brought over from node1 is online and set to root:
+
`aggr status`
+
NOTE: Failing to perform the previous substep might cause node3 to boot from the internal root aggregate, or it might cause the system to assume a new cluster configuration exists or prompt you to identify one.

+
The following shows an example of the command output:

+
----
 ---------------------------------------------------------------
      Aggr State               Status          Options
 aggr0_nst_fas8080_15 online   raid_dp, aggr   root, nosnap=on
                               fast zeroed
                               64-bit

   aggr0 offline               raid_dp, aggr   diskroot
                               fast zeroed
                               64-bit
 ----------------------------------------------------------------------
----

. [[man_install3_step33]]Verify that the controller and chassis are configured as `ha`:
+
`ha-config show`
+
The following example shows the output of the ha-config show command:
+
----
 *> ha-config show
    Chassis HA configuration: ha
    Controller HA configuration: ha
----
+
Systems record in a programmable ROM (PROM) whether they are in an HA pair or stand-alone configuration. The state must be the same on all components within the stand-alone system or HA pair.
+
If the controller and chassis are not configured as "ha", use the following commands to correct the configuration:
+
`ha-config modify controller ha`
+
`ha-config modify chassis ha`
+
If you have a MetroCluster configuration, use the following commands to modify the controller and chassis:
+
`ha-config modify controller mcc`
+
`ha-config modify chassis mcc`

. [[man_install3_step34]]Destroy the mailboxes on node3:
+
`mailbox destroy local`
+
The console displays the following message:
+
----
Destroying mailboxes forces a node to create new empty mailboxes, which clears any takeover state, removes all knowledge of out-of-date plexes of mirrored volumes, and will prevent management services from going online in 2-node cluster HA configurations. Are you sure you want to destroy the local mailboxes?
----

. [[man_install3_step35]]Enter `y` at the prompt to confirm that you want to destroy the local mailboxes.

. [[man_install3_step36]]Exit maintenance mode:
+
`halt`
+
The system stops at the boot environment prompt.

. [[man_install3_step37]]On node2, check the system date, time, and time zone:
+
`date`

. [[man_install3_step38]]On node3, check the date at the boot environment prompt:
+
`show date`

. [[man_install3_step39]]If necessary, set the date on node3:
+
`set date _mm/dd/yyyy_`

. [[man_install3_step40]]On node3, check the time at the boot environment prompt:
+
`show time`

. [[man_install3_step41]]If necessary, set the time on node3:
+
`set time _hh:mm:ss_`

. [[man_install3_step42]]Verify the partner system ID is set correctly as noted in <<man_install3_step28,Step 28>> under -p switch:
+
`printenv partner-sysid`

. [[man_install3_step43]]If necessary, set the partner system ID on node3:
+
`setenv partner-sysid _node2_sysid_`
+
Save the settings:
+
`saveenv`

. [[man_install3_step44]]Access the boot menu at the boot environment prompt:
+
`boot_ontap menu`

. [[man_install3_step45]]At the boot menu, select option *(6) Update flash from backup config* by entering `6` at the prompt.
+
The system displays the following message:
+
----
This will replace all flash-based configuration with the last backup to disks. Are you sure you want to continue?:
----

. [[man_install3_step46]]Enter `y` at the prompt.
+
The boot proceeds normally, and the system then asks you to confirm the system ID mismatch.
+
NOTE: The system might reboot twice before displaying the mismatch warning.

. [[man_install3_step47]]Confirm the mismatch as shown in the following example:
+
----
WARNING: System id mismatch. This usually occurs when replacing CF or NVRAM cards!
Override system id (y|n) ? [n] y
----
+
The node might go through one round of reboot before booting normally.

. [[man_install3_step48]]Log in to node3.


// 2023 Feb 22, BURT 1518041
// 2022 MAY 13, BURT 1478241
// 2022 MAR 09, Clean-up
// 2021 FEB 22, Formatted from CMS
// 2022 Sep 28, BURT 1501272

